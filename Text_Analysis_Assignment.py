{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "27c74c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fetch the HTML content of a URL\n",
    "def fetch_html_content(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            return response.text\n",
    "        else:\n",
    "            print(f\"Failed to fetch content from {url}. Status code: {response.status_code}\")\n",
    "            return None\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Failed to fetch content from {url}. Error: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7b4530b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_html_content(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise an error if response status is not 200\n",
    "        return response.text\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Failed to fetch content from {url}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a5c8d970",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_article_data(html_content):\n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "    article_title_element = soup.find(\"h1\")\n",
    "    article_text_element = soup.find(\"div\", class_=\"article-content\")\n",
    "    article_title = article_title_element.text.strip() if article_title_element else \"\"\n",
    "    article_text = article_text_element.text.strip() if article_text_element else \"\"\n",
    "    return article_title, article_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8fe18049",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentimental_analysis(text):\n",
    "    positive_words = set()\n",
    "    negative_words = set()\n",
    "\n",
    "    # Provide the full path to positive-words.txt and negative-words.txt\n",
    "    positive_words_file = \"/Users/vikranthreddimasu/Downloads/Assignment/positive-words.txt\"\n",
    "    negative_words_file = \"/Users/vikranthreddimasu/Downloads/Assignment/negative-words.txt\"\n",
    "\n",
    "    with open(positive_words_file, \"r\", encoding=\"latin-1\") as file:\n",
    "        positive_words.update(file.read().splitlines())\n",
    "\n",
    "    with open(negative_words_file, \"r\", encoding=\"latin-1\") as file:\n",
    "        negative_words.update(file.read().splitlines())\n",
    "\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    positive_score = sum(1 for word in tokens if word in positive_words)\n",
    "    negative_score = sum(1 for word in tokens if word in negative_words)\n",
    "\n",
    "    if positive_score + negative_score == 0:\n",
    "        polarity_score = 0\n",
    "        subjectivity_score = 0\n",
    "    else:\n",
    "        polarity_score = (positive_score - negative_score) / (positive_score + negative_score + 0.000001)\n",
    "        subjectivity_score = (positive_score + negative_score) / (len(tokens) + 0.000001)\n",
    "\n",
    "    return positive_score, negative_score, polarity_score, subjectivity_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5a95bbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_analysis(text):\n",
    "    words = word_tokenize(text)\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "\n",
    "    avg_sentence_length = len(words) / len(sentences)\n",
    "    complex_words = [word for word in words if len(word) > 2 and word.isalpha()]\n",
    "    percentage_complex_words = len(complex_words) / len(words)\n",
    "    fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)\n",
    "    avg_words_per_sentence = len(words) / len(sentences)\n",
    "    complex_word_count = len(complex_words)\n",
    "    word_count = len(words)\n",
    "    syllable_count_per_word = sum([nltk.syllable_count(word) for word in words]) / len(words)\n",
    "    personal_pronouns = [\"i\", \"we\", \"my\", \"ours\", \"us\"]\n",
    "    personal_pronouns_count = sum(1 for word in words if word.lower() in personal_pronouns)\n",
    "    total_chars = sum(len(word) for word in words)\n",
    "    avg_word_length = total_chars / len(words)\n",
    "\n",
    "    return (\n",
    "        avg_sentence_length,\n",
    "        percentage_complex_words,\n",
    "        fog_index,\n",
    "        avg_words_per_sentence,\n",
    "        complex_word_count,\n",
    "        word_count,\n",
    "        syllable_count_per_word,\n",
    "        personal_pronouns_count,\n",
    "        avg_word_length,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1711d0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_stop_words():\n",
    "    stop_words_files = [\n",
    "        \"/Users/vikranthreddimasu/Downloads/Assignment/StopWords_Names.txt\",\n",
    "        \"/Users/vikranthreddimasu/Downloads/Assignment/StopWords_Geographic.txt\",\n",
    "        \"/Users/vikranthreddimasu/Downloads/Assignment/StopWords_GenericLong.txt\",\n",
    "        \"/Users/vikranthreddimasu/Downloads/Assignment/StopWords_Generic.txt\",\n",
    "        \"/Users/vikranthreddimasu/Downloads/Assignment/StopWords_DatesandNumbers.txt\",\n",
    "        \"/Users/vikranthreddimasu/Downloads/Assignment/StopWords_Currencies.txt\",\n",
    "        \"/Users/vikranthreddimasu/Downloads/Assignment/StopWords_Auditor.txt\"\n",
    "    ]\n",
    "\n",
    "    stop_words = set()\n",
    "\n",
    "    for file_name in stop_words_files:\n",
    "        with open(file_name, \"r\", encoding=\"latin-1\") as file:\n",
    "            words = file.read().splitlines()\n",
    "            stop_words.update(words)\n",
    "\n",
    "    return stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d937de7f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 46\u001b[0m\n\u001b[1;32m     33\u001b[0m         positive_score, negative_score, polarity_score, subjectivity_score \u001b[38;5;241m=\u001b[39m sentimental_analysis(article_text)\n\u001b[1;32m     35\u001b[0m         \u001b[38;5;66;03m# Text analysis\u001b[39;00m\n\u001b[1;32m     36\u001b[0m         (\n\u001b[1;32m     37\u001b[0m             avg_sentence_length,\n\u001b[1;32m     38\u001b[0m             percentage_complex_words,\n\u001b[1;32m     39\u001b[0m             fog_index,\n\u001b[1;32m     40\u001b[0m             avg_words_per_sentence,\n\u001b[1;32m     41\u001b[0m             complex_word_count,\n\u001b[1;32m     42\u001b[0m             word_count,\n\u001b[1;32m     43\u001b[0m             syllable_count_per_word,\n\u001b[1;32m     44\u001b[0m             personal_pronouns_count,\n\u001b[1;32m     45\u001b[0m             avg_word_length,\n\u001b[0;32m---> 46\u001b[0m         ) \u001b[38;5;241m=\u001b[39m text_analysis(article_text)\n\u001b[1;32m     48\u001b[0m         results_df \u001b[38;5;241m=\u001b[39m results_df\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m     49\u001b[0m             {\n\u001b[1;32m     50\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mURL_ID\u001b[39m\u001b[38;5;124m\"\u001b[39m: row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mURL_ID\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     68\u001b[0m             ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     69\u001b[0m         )\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# Display the results DataFrame\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[48], line 5\u001b[0m, in \u001b[0;36mtext_analysis\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      2\u001b[0m words \u001b[38;5;241m=\u001b[39m word_tokenize(text)\n\u001b[1;32m      3\u001b[0m sentences \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39msent_tokenize(text)\n\u001b[0;32m----> 5\u001b[0m avg_sentence_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(words) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(sentences)\n\u001b[1;32m      6\u001b[0m complex_words \u001b[38;5;241m=\u001b[39m [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(word) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m word\u001b[38;5;241m.\u001b[39misalpha()]\n\u001b[1;32m      7\u001b[0m percentage_complex_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(complex_words) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(words)\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "# Load stop words\n",
    "stop_words = load_stop_words()\n",
    "\n",
    "# Create a new DataFrame to store the results\n",
    "results_df = pd.DataFrame(columns=[\n",
    "    \"URL_ID\",\n",
    "    \"URL\",\n",
    "    \"Article Title\",\n",
    "    \"Article Text\",\n",
    "    \"Positive Score\",\n",
    "    \"Negative Score\",\n",
    "    \"Polarity Score\",\n",
    "    \"Subjectivity Score\",\n",
    "    \"Avg Sentence Length\",\n",
    "    \"Percentage of Complex Words\",\n",
    "    \"Fog Index\",\n",
    "    \"Avg Words per Sentence\",\n",
    "    \"Complex Word Count\",\n",
    "    \"Word Count\",\n",
    "    \"Syllable per Word\",\n",
    "    \"Personal Pronouns\",\n",
    "    \"Avg Word Length\",\n",
    "])\n",
    "\n",
    "# Loop through the URLs, fetch the HTML content, and extract the article data\n",
    "for index, row in df_urls.iterrows():\n",
    "    url = row[\"URL\"]\n",
    "    html_content = fetch_html_content(url)\n",
    "    if html_content:\n",
    "        article_title, article_text = extract_article_data(html_content)\n",
    "\n",
    "        # Sentimental analysis\n",
    "        positive_score, negative_score, polarity_score, subjectivity_score = sentimental_analysis(article_text)\n",
    "\n",
    "        # Text analysis\n",
    "        (\n",
    "            avg_sentence_length,\n",
    "            percentage_complex_words,\n",
    "            fog_index,\n",
    "            avg_words_per_sentence,\n",
    "            complex_word_count,\n",
    "            word_count,\n",
    "            syllable_count_per_word,\n",
    "            personal_pronouns_count,\n",
    "            avg_word_length,\n",
    "        ) = text_analysis(article_text)\n",
    "\n",
    "        results_df = results_df.append(\n",
    "            {\n",
    "                \"URL_ID\": row[\"URL_ID\"],\n",
    "                \"URL\": url,\n",
    "                \"Article Title\": article_title,\n",
    "                \"Article Text\": article_text,\n",
    "                \"Positive Score\": positive_score,\n",
    "                \"Negative Score\": negative_score,\n",
    "                \"Polarity Score\": polarity_score,\n",
    "                \"Subjectivity Score\": subjectivity_score,\n",
    "                \"Avg Sentence Length\": avg_sentence_length,\n",
    "                \"Percentage of Complex Words\": percentage_complex_words,\n",
    "                \"Fog Index\": fog_index,\n",
    "                \"Avg Words per Sentence\": avg_words_per_sentence,\n",
    "                \"Complex Word Count\": complex_word_count,\n",
    "                \"Word Count\": word_count,\n",
    "                \"Syllable per Word\": syllable_count_per_word,\n",
    "                \"Personal Pronouns\": personal_pronouns_count,\n",
    "                \"Avg Word Length\": avg_word_length,\n",
    "            },\n",
    "            ignore_index=True,\n",
    "        )\n",
    "\n",
    "# Display the results DataFrame\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17832314",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
